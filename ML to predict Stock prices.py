# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FTrZZyP_kH6yR7uXOMIgjBnndq2nsg2z
"""

# this program use LSTM to predict the closing stock price (Apple.inc ) using the past 60 day stock price

## import libaries

import math
import pandas_datareader as web
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# get the stock quote 
# questions, use help to find out the data source and what else we are available to choose from # 
df= web.DataReader('AAPL', data_source='yahoo', start= '2012-01-01', end= '2019-12-17')
df

# get the unmber of the rows and columns 
df.shape

# visualise the closing rpice history
# starting with giving our figure a size 
plt.figure(figsize=(16,8))
# then we want to give our plot a title
plt.title('Closing Price History')
# then we want to give our plt a data which is the close column of the df
plt.plot(df['Close'])
plt.xlabel('Date',fontsize=18)
plt.ylabel('Close Price USD $')
plt.show()

# create a new datafram with onlyt the close column 
data = df.filter(['Close'])
#conver the dataframe to a numpy array 
dataset= data.values
#get the number of rows to train the model on 
training_data_len = math.ceil(len(dataset) * .8)

#Scale the data
# As research shows that it is advantegous to use the pre-processing transformations and noramlisation to the input before it is presented
# to the neural networks 

scaler = MinMaxScaler(feature_range=(0,1))

# after create a scaler, you want to imput the scaler to fit and then transform the data

scaled_data = scaler.fit_transform(dataset)

scaled_data

# create the training dataset
# create the scaled training dataset 
#?? i know python is row first, but why we need to address column here ? 
train_data= scaled_data[0: training_data_len , :]

# split the data into x_train and y_train datasets
x_train = []
y_train = [ ]

# give it the range of 60 numbers and also the list of where those 60 comes from 

for i in range (60, len(train_data)):
  x_train.append(train_data[i-60:i, 0])
  y_train.append(train_data[i, 0])

  if i<= 60:
    print(x_train)
    print(y_train)



#convert the x y train to numpy arrays

x_train, y_train = np.array(x_train), np.array(y_train)

#reshape the data
#LSTM expects data to be three dimensional and right now we only have the only with two
# the extra value we added here is the number of feature, which is only 1 refer to the closing price

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))

x_train.shape

# build the LSTM model
# create the model variable 
 model = Sequential()
 
# adding a layer to our model, ltsm give the 50 layers of neurons, return sequence we want it 
# to be true as we need to add another LSTM , input shape will be the number of time steps 

 model.add(LSTM(50,return_sequences= True, input_shape= (x_train.shape[1],1)))

 # then we do another layer of 50, but this time we choose return sequences as false we are not adding more LSTM layers  
 model.add(LSTM(50, return_sequences= False) )

 # then we add anpther Densely connected neural networks layer
model.add (Dense(25))
model.add (Dense(1))

# complie the model 
model.compile(optimizer = 'adam', loss='mean_squared_error')

# train the model 
# batch size is the number of training examples in the single batch 
# epochs is the number of iteration when the whole model is running and passing through

model.fit(x_train, y_train, batch_size=1, epochs=1)

# creat the teting data set 
# create a new array containing scaled values from index 1543 to 2003 
test_data = scaled_data[training_data_len -60:, :]

# create the data sets x_test and y_test 
x_test = []
# this is defined as we already give the value of training_data_len * 0.8 as the training dataset
y_test=dataset[training_data_len:, :]

for i in range(60, len(test_data)):

  #we want to append the test_data from i-60 to i, but i is not inclusive, so we use cooma here. And we want to use 0 as the column...
  x_test.append(test_data[i-60:i, 0])

# convert data to numpy 

x_test = np.array(x_test)

# reshape the data 
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

# get the model predicted pricevalue 

predictions = model.predict(x_test)   
#unscaling the values and make the predictions contain the same value of y_test.dataset 

predictions = scaler.inverse_transform(predictions)

# evaluate our models
# we can achieve that by get the root mean sqaured error (RMSE), we want to get small RMSE
# we could also get the RMSE from other different models and different initution of the model
rmse=np.sqrt(np.mean(((predictions- y_test)**2)))
rmse

# plot the data
 train = data[: training_data_len]
 valid = data[training_data_len :]
 valid['Predictions']= predictions 
 #visualise the data 
 plt.figure(figsize= (16,8))
 plt.title('Model')
 plt.xlabel('Date', fontsize=18)
 plt.ylabel('Close Price USD($)', fontsize=18)
 plt.plot(train['Close'])
 plt.plot(valid[['Close', 'Predictions']])
 plt.legend(['Tri', 'Val', 'Predictions'], loc= 'lower right')
 plt.show()

